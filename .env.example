# LLM Configuration
# Choose one provider: OpenAI, Anthropic, Watsonx, or OpenRouter

# OpenRouter Configuration (Recommended - Free tier available)
OPENROUTER_API_KEY=your_openrouter_api_key_here

# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic Configuration
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# IBM Watsonx Configuration
WATSONX_API_KEY=your_watsonx_api_key_here
WATSONX_PROJECT_ID=your_watsonx_project_id_here
WATSONX_URL=us-south.ml.cloud.ibm.com

# LLM Provider: "openai", "anthropic", "watsonx", or "openrouter"
LLM_PROVIDER=openrouter

# Model Configuration
# For OpenAI: gpt-4-turbo-preview, gpt-4, gpt-3.5-turbo
# For Anthropic: claude-3-opus-20240229, claude-3-sonnet-20240229
# For Watsonx: ibm/granite-3-3-8b-instruct, ibm/granite-13b-instruct, etc.
# For OpenRouter: deepseek/deepseek-chat-v3.1:free, meta-llama/llama-3.1-8b-instruct:free, etc.
MODEL_NAME=deepseek/deepseek-chat-v3.1:free

# Temperature (0.0 to 1.0) - lower is more deterministic
TEMPERATURE=0.5

# Max tokens for response (increased for code reviews)
MAX_TOKENS=500

# GitHub Configuration (optional, for PR reviews)
GITHUB_TOKEN=your_github_token_here
